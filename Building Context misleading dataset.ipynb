{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "H9EYmUkP9iQ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQJ1iTKk71cu"
      },
      "outputs": [],
      "source": [
        "!sudo apt install tesseract-ocr -y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat spacytextblob transformers"
      ],
      "metadata": {
        "id": "mN7nHc9h9nQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Dependencies"
      ],
      "metadata": {
        "id": "kPUaKJVP903b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import socket\n",
        "\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "\n",
        "import urllib\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "from http.client import HTTPConnection, HTTPSConnection\n",
        "\n",
        "import re\n",
        "\n",
        "import spacy\n",
        "import urllib.request\n",
        "\n",
        "from unicodedata import normalize\n",
        "import cv2\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead"
      ],
      "metadata": {
        "id": "mVMhPfz19tTT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Deceptive-Research/annotation.zip"
      ],
      "metadata": {
        "id": "a_JWR6t_93zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "nlp.add_pipe('spacytextblob')"
      ],
      "metadata": {
        "id": "t-GKFes2-Fet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the text Data\n"
      ],
      "metadata": {
        "id": "XrOCfr5Y-SZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    text = normalize(\"NFKD\", text) #Normalization\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\",\"\", text) #Remove Punc\n",
        "\n",
        "    # text = \" \".join([token.lemma_ for token in nlp(text) if not token.is_stop])\n",
        "\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "olnfs66g-QFT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the image"
      ],
      "metadata": {
        "id": "-s1m2FfF-jU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_img(img):\n",
        "\n",
        "    img = cv2.resize(img, (300, 300))\n",
        "\n",
        "    # Normalization\n",
        "    img = img/255.0\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "PNFBl4NY-dDc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features Extraction"
      ],
      "metadata": {
        "id": "JkFtritOAMY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize the webpage using T5"
      ],
      "metadata": {
        "id": "sOxNLGRd-yfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained('T5-base')\n",
        "model=AutoModelWithLMHead.from_pretrained('T5-base', return_dict=True)"
      ],
      "metadata": {
        "id": "roa1E6o6_Jl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_webpage(sequence):\n",
        "\n",
        "  inputs = tokenizer.encode(\"sumarize: \" +sequence,return_tensors='np', max_length=512, truncation=True)\n",
        "  output = model.generate(inputs, min_length=80, max_length=100)\n",
        "\n",
        "  summary = tokenizer.decode(output[0])\n",
        "\n",
        "  return summary"
      ],
      "metadata": {
        "id": "VIliSkpi-xFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OCR - Image to Text"
      ],
      "metadata": {
        "id": "O7ceie_b_scl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(image):\n",
        "\n",
        "    txt = pytesseract.image_to_string(image, lang=\"eng\")\n",
        "    txt = re.sub(\"[\\n]{2,}\", \"\\t\\t\", txt)\n",
        "    txt = re.sub(\"\\n\", \"\", txt)\n",
        "    txt = re.sub(\"\\t\\t\", \"\\n\", txt)\n",
        "\n",
        "    if not txt:\n",
        "      txt = \"No Information\"\n",
        "\n",
        "    return txt"
      ],
      "metadata": {
        "id": "B3_lM9Xw-o1O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity"
      ],
      "metadata": {
        "id": "GxOByiXx_7Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(text1, text2):\n",
        "\n",
        "  doc1 = nlp(text1)\n",
        "  doc2 = nlp(text2)\n",
        "\n",
        "  return doc1.similarity(doc2)"
      ],
      "metadata": {
        "id": "UsM2m5ri_5YC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Sentiment"
      ],
      "metadata": {
        "id": "ItPb5LlqAElv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(text):\n",
        "\n",
        "  doc = nlp(text)\n",
        "\n",
        "  return doc._.blob.polarity"
      ],
      "metadata": {
        "id": "Rht7nxw3AC4T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Is Secure"
      ],
      "metadata": {
        "id": "UwtqMWc4Bo-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_https_url(url):\n",
        "    HTTPS_URL = f'https://{url}'\n",
        "    try:\n",
        "        HTTPS_URL = urlparse(HTTPS_URL)\n",
        "        connection = HTTPSConnection(HTTPS_URL.netloc, timeout=2)\n",
        "        connection.request('HEAD', HTTPS_URL.path)\n",
        "        if connection.getresponse():\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    except:\n",
        "        return 0\n",
        ""
      ],
      "metadata": {
        "id": "K81gvV5oBnUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Host Name Feature"
      ],
      "metadata": {
        "id": "XkxksSOABwIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_host(url):\n",
        "  res = re.findall(\"^(www.|https://|http://)\", url)\n",
        "  if res:\n",
        "    url = re.sub(f\"^{res[0]}\", \"\", url)\n",
        "  try:\n",
        "    socket.gethostbyname(url)\n",
        "    return 1\n",
        "  except:\n",
        "    return 0\n",
        ""
      ],
      "metadata": {
        "id": "HC9M1-QJBuUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Active/Inactive"
      ],
      "metadata": {
        "id": "jHX4O3QtB66T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_active(url):\n",
        "\n",
        "  url = url if url[:8] in [\"http://\", \"https://\"] else \"http://\"+url\n",
        "  try:\n",
        "    r = requests.head(url, timeout=3)\n",
        "\n",
        "    if r.status_code == 200: return 1\n",
        "    else: return 0\n",
        "  except: return 0\n"
      ],
      "metadata": {
        "id": "NnHDbL1_B4p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check number of Re-directions a URL has"
      ],
      "metadata": {
        "id": "f0wDMW7YCDQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_redirect(url):\n",
        "  try:\n",
        "    url = url if url[:8] in [\"http://\", \"https://\"] else \"http://\"+url\n",
        "    r = requests.get(url, timeout=3)\n",
        "    return len(r.history)\n",
        "  except:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "ZE-bJwcICB-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Annotated Data Extraction and derive the features."
      ],
      "metadata": {
        "id": "L_kXoseGAYGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(soup):\n",
        "\n",
        "  imgs = []\n",
        "  img_txts = []\n",
        "  y_true = []\n",
        "  cos_sims = []\n",
        "\n",
        "\n",
        "  text = summarize_webpage(soup)\n",
        "\n",
        "  # find all images in URL\n",
        "  images = soup.findAll('img', alt=True)\n",
        "\n",
        "  # checking if images is not zero\n",
        "  if len(images) != 0:\n",
        "    for i, image in enumerate(images):\n",
        "\n",
        "      try:\n",
        "        image_link = image[\"data-srcset\"]\n",
        "\n",
        "      except:\n",
        "        try:\n",
        "\n",
        "          image_link = image[\"data-src\"]\n",
        "\n",
        "        except:\n",
        "          try:\n",
        "\n",
        "            image_link = image[\"data-fallback-src\"]\n",
        "          except:\n",
        "            try:\n",
        "\n",
        "              image_link = image[\"src\"]\n",
        "\n",
        "            except Exception as e:\n",
        "              print(f\"Error: {e}\")\n",
        "\n",
        "      try:\n",
        "\n",
        "        alt_text = image[\"alt\"]\n",
        "\n",
        "        if alt_text in [\"deceptive\", \"normal\"]:\n",
        "\n",
        "          y = 1 if alt_text == \"deceptive\" else 0\n",
        "\n",
        "          if Path(image_link).suffix ==  \".svg\":\n",
        "            img_png = cairosvg.svg2png(url = image_link)\n",
        "            img = plt.imread(BytesIO(img_png))[:,:,:3]\n",
        "            img = np.array(Image.fromarray((img * 255).astype(np.uint8)))\n",
        "\n",
        "          else: img = plt.imread(image_link)\n",
        "\n",
        "          img_txt = get_data(img)\n",
        "          img = clean_img(img)\n",
        "          cos_sim = cos_similarity(text, img_txt)\n",
        "          sentiment = sentiment(text)\n",
        "          read = readability(text)\n",
        "\n",
        "          imgs.append(img)\n",
        "          img_txts.append(img_txt)\n",
        "          cos_sims.append(cos_sim)\n",
        "          y_true.append(y)\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "  return imgs, img_txts, cos_sims, y_true, sentiment, read"
      ],
      "metadata": {
        "id": "4wVqvdjiAJAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function to build the dataset"
      ],
      "metadata": {
        "id": "hxafuPfwAuyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(f):\n",
        "\n",
        "  with open(f, \"rb\") as f:\n",
        "    html = f.read().decode('utf-8')\n",
        "\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  # Call folder create function\n",
        "  return extract_data(soup)\n",
        "\n",
        "\n",
        "os.chdir(\"/content/annotation\")\n",
        "\n",
        "X_img = []\n",
        "X_txt = []\n",
        "X_cos = []\n",
        "y = []\n",
        "sentiment =[]\n",
        "read_score = []\n",
        "\n",
        "for f in list(Path('/content/annotation').glob('*.html')):\n",
        "  img, txt, cos_sim, label, polarity, score = main(f)\n",
        "  X_img.extend(img)\n",
        "  X_txt.extend(txt)\n",
        "  X_cos.extend(cos_sim)\n",
        "  y.extend(label)\n",
        "  sentiment.extend(polarity)\n",
        "  read_score.extend(score)"
      ],
      "metadata": {
        "id": "Mcmy9Z5LAtCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extras"
      ],
      "metadata": {
        "id": "yv0yxVu_BHDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "id": "kmzFCDwNBLUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cos"
      ],
      "metadata": {
        "id": "xkVu_8TABNDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/annotation/What is The Rock Workout Routine_ - SET FOR SET.html\", \"r\") as f:\n",
        "  html = f.read()\n",
        "\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "# get text\n",
        "text = soup.get_text()\n",
        "\n",
        "# break into lines and remove leading and trailing space on each\n",
        "lines = (line.strip() for line in text.splitlines())\n",
        "\n",
        "# break multi-headlines into a line each\n",
        "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "\n",
        "# drop blank lines\n",
        "text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "print(text)\n",
        ""
      ],
      "metadata": {
        "id": "KkE8cTraBFTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}