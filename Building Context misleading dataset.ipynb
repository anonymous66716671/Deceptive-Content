{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "931XDu7oZMC5"
      },
      "outputs": [],
      "source": [
        "!sudo apt install tesseract-ocr -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW088egJthSV"
      },
      "outputs": [],
      "source": [
        "!pip install cairosvg pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ChtpLkxhf_"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6LPeTRGKh3i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import requests\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import cairosvg\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "import pytesseract\n",
        "\n",
        "import spacy\n",
        "from unicodedata import normalize\n",
        "\n",
        "import cv2\n",
        "\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e73YEscXF6lN"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/Deceptive-Research/annotation.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcHfh_GR2nmx"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sMZvlksd2bqU"
      },
      "source": [
        "### Cleaning the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNiYJo0N2ab5"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    text = normalize(\"NFKD\", text) #Normalization\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\",\"\", text) #Remove Punc\n",
        "\n",
        "    # text = \" \".join([token.lemma_ for token in nlp(text) if not token.is_stop])\n",
        "\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bD2ZZmfAVEFv"
      },
      "source": [
        "### Cleaning the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBXHrAP4VDJ5"
      },
      "outputs": [],
      "source": [
        "def clean_img(img):\n",
        "\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # Normalization\n",
        "    img = img/255.0\n",
        "\n",
        "    return img"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pDHO32pQxuTs"
      },
      "source": [
        "### Importing Open API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHvBsP5_xwwb"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/openai_api_key.txt\", \"r\") as f:\n",
        "  openai.api_key = f.read()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lgCi_jxFx2Ke"
      },
      "source": [
        "### ChatGPT Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90aY-k2Px0IH"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9-ZxvMMa13SG"
      },
      "source": [
        "### Summarize the webpage using ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUnpOyZ6yZWX"
      },
      "outputs": [],
      "source": [
        "def summarize_webpage(website_link):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Your task is to summarize the website.\n",
        "\n",
        "  Website: ```{website_link}```\n",
        "  \"\"\"\n",
        "  response = get_completion(prompt)\n",
        "\n",
        "  return response"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-SNVwDnS16ZI"
      },
      "source": [
        "### OCR - Image to Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On9o-neH18qE"
      },
      "outputs": [],
      "source": [
        "def get_data(image):\n",
        "\n",
        "    txt = pytesseract.image_to_string(image, lang=\"eng\")\n",
        "    txt = re.sub(\"[\\n]{2,}\", \"\\t\\t\", txt)\n",
        "    txt = re.sub(\"\\n\", \"\", txt)\n",
        "    txt = re.sub(\"\\t\\t\", \"\\n\", txt)\n",
        "\n",
        "    if not txt:\n",
        "      txt = \"No Information\"\n",
        "\n",
        "    return txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bciLKdyFXimz"
      },
      "source": [
        "### Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP4r-sW2Xk6H"
      },
      "outputs": [],
      "source": [
        "def cos_similarity(text1, text2):\n",
        "\n",
        "  doc1 = nlp(text1)\n",
        "  doc2 = nlp(text2)\n",
        "\n",
        "  return doc1.similarity(doc2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wXQX-JOc3vTW"
      },
      "source": [
        "### Annotated Deceptive Data Extrcation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ynIDHX6E5X"
      },
      "outputs": [],
      "source": [
        "def extract_data(soup):\n",
        "\n",
        "  imgs = []\n",
        "  img_txts = []\n",
        "  y_true = []\n",
        "  cos_sims = []\n",
        "\n",
        "\n",
        "  text = summarize_webpage(soup)\n",
        "\n",
        "  # find all images in URL\n",
        "  images = soup.findAll('img', alt=True)\n",
        "\n",
        "  # checking if images is not zero\n",
        "  if len(images) != 0:\n",
        "    for i, image in enumerate(images):\n",
        "\n",
        "      try:\n",
        "        image_link = image[\"data-srcset\"]\n",
        "\n",
        "      except:\n",
        "        try:\n",
        "      \n",
        "          image_link = image[\"data-src\"]\n",
        "\n",
        "        except:\n",
        "          try:\n",
        "\n",
        "            image_link = image[\"data-fallback-src\"]\n",
        "          except:\n",
        "            try:\n",
        "          \n",
        "              image_link = image[\"src\"]\n",
        "\n",
        "            except Exception as e:\n",
        "              print(f\"Error: {e}\")\n",
        "\n",
        "      try:\n",
        "\n",
        "        alt_text = image[\"alt\"]\n",
        "\n",
        "        if alt_text in [\"deceptive\", \"normal\"]:\n",
        "\n",
        "          y = 1 if alt_text == \"deceptive\" else 0\n",
        "\n",
        "          if Path(image_link).suffix ==  \".svg\":\n",
        "            img_png = cairosvg.svg2png(url = image_link)\n",
        "            img = plt.imread(BytesIO(img_png))[:,:,:3]\n",
        "            img = np.array(Image.fromarray((img * 255).astype(np.uint8)))\n",
        "            \n",
        "          else: img = plt.imread(image_link)\n",
        "          \n",
        "          img_txt = get_data(img)\n",
        "          img = clean_img(img)\n",
        "          cos_sim = cos_similarity(text, img_txt)\n",
        "\n",
        "          imgs.append(img)\n",
        "          img_txts.append(img_txt)\n",
        "          cos_sims.append(cos_sim)\n",
        "          y_true.append(y)\n",
        "            \n",
        "      except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "  return imgs, img_txts, cos_sims, y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9tNTIg47WJc"
      },
      "outputs": [],
      "source": [
        "def main(f):\n",
        "\n",
        "  with open(f, \"rb\") as f:\n",
        "    html = f.read().decode('utf-8')\n",
        "  \n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  # Call folder create function\n",
        "  return extract_data(soup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9lTEVtGMmbg",
        "outputId": "8e0d7b51-575d-4946-ccbf-949af90f11ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-ea8c808ccdcd>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  return doc1.similarity(doc2)\n",
            "<ipython-input-8-ea8c808ccdcd>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  return doc1.similarity(doc2)\n",
            "<ipython-input-8-ea8c808ccdcd>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  return doc1.similarity(doc2)\n",
            "<ipython-input-8-ea8c808ccdcd>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  return doc1.similarity(doc2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Cannot handle this data type: (1, 1, 3), <f4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-ea8c808ccdcd>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  return doc1.similarity(doc2)\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"/content/annotation\")\n",
        "\n",
        "X_img = []\n",
        "X_txt = []\n",
        "X_cos = []\n",
        "y = []\n",
        "\n",
        "for f in list(Path('/content/annotation').glob('*.html')):\n",
        "  img, txt, cos_sim, label = main(f)\n",
        "  X_img.extend(img)\n",
        "  X_txt.extend(txt)\n",
        "  X_cos.extend(cos_sim)\n",
        "  y.extend(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6US_cCy0tNyP",
        "outputId": "2255bc2d-801f-4d36-db5b-b3192dee473c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtRxQgW3tQMp",
        "outputId": "2f1005c7-be7a-4d84-b553-2481fc71e268"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.03491137395281285,\n",
              " -0.1219453025589311,\n",
              " 0.025956913693130596,\n",
              " 0.09302007141181075,\n",
              " 0.15937664581111233]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_cos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PfLNxl0cyJnu"
      },
      "source": [
        "### Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bn8SmlYvmmh"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/annotation/What is The Rock Workout Routine_ - SET FOR SET.html\", \"r\") as f:\n",
        "  html = f.read()\n",
        "\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "# get text\n",
        "text = soup.get_text()\n",
        "\n",
        "# break into lines and remove leading and trailing space on each\n",
        "lines = (line.strip() for line in text.splitlines())\n",
        "\n",
        "# break multi-headlines into a line each\n",
        "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "\n",
        "# drop blank lines\n",
        "text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySnrCsFdxPHY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO6VclAKqgIWVLNPatrXl6o",
      "include_colab_link": true,
      "mount_file_id": "1htOGmW2yppDdi3ZPwCUyQ5iwqmjnh1Zl",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
